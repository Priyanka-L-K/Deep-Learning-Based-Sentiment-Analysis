{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mQ2L50qv6wu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class RBMs(object):\n",
        "    \n",
        "    def __init__(self, visibleDimensions, epochs=20, hiddenDimensions=50, ratingValues=10, learningRate=0.001, batchSize=100):\n",
        "       self.visibleDimensions = visibleDimensions\n",
        "       self.epochs = epochs\n",
        "       self.hiddenDimensions = hiddenDimensions\n",
        "       self.ratingValues = ratingValues\n",
        "       self.learningRate = learningRate\n",
        "       self.batchSize = batchSize\n",
        "    def Train(self, X):\n",
        "       for epoch in range(self.epochs):\n",
        "            np.random.shuffle(X)\n",
        "            \n",
        "            trX = np.array(X)\n",
        "            for i in range(0, trX.shape[0], self.batchSize):\n",
        "                epochX = trX[i:i+self.batchSize]\n",
        "                self.MakeGraph(epochX)\n",
        "\n",
        "            print(\"Trained epoch \", epoch)\n",
        "    def GetRecommendations(self, inputUser):\n",
        "       feed = self.MakeHidden(inputUser)\n",
        "       rec = self.MakeVisible(feed)\n",
        "       return rec[0]  \n",
        "    def MakeGraph(self, inputUser):\n",
        "        maxWeight = -4.0 * np.sqrt(6.0 / (self.hiddenDimensions + self.visibleDimensions))\n",
        "        self.weights = tf.Variable(tf.random.uniform([self.visibleDimensions, self.hiddenDimensions], minval=-maxWeight, maxval=maxWeight), tf.float32, name=\"weights\")\n",
        "        \n",
        "        self.hiddenBias = tf.Variable(tf.zeros([self.hiddenDimensions], tf.float32, name=\"hiddenBias\"))\n",
        "        self.visibleBias = tf.Variable(tf.zeros([self.visibleDimensions], tf.float32, name=\"visibleBias\"))\n",
        "        \n",
        "        # Perform Gibbs Sampling for Contrastive Divergence, per the paper we assume k=1 instead of iterating over the \n",
        "        # forward pass multiple times since it seems to work just fine\n",
        "        \n",
        "        # Forward pass\n",
        "        # Sample hidden layer given visible...\n",
        "        # Get tensor of hidden probabilities\n",
        "        hProb0 = tf.nn.sigmoid(tf.matmul(inputUser, self.weights) + self.hiddenBias)\n",
        "        # Sample from all of the distributions\n",
        "        hSample = tf.nn.relu(tf.sign(hProb0 - tf.random.uniform(tf.shape(hProb0))))\n",
        "        # Stitch it together\n",
        "        forward = tf.matmul(tf.transpose(inputUser), hSample)\n",
        "        \n",
        "        # Backward pass\n",
        "        # Reconstruct visible layer given hidden layer sample\n",
        "        v = tf.matmul(hSample, tf.transpose(self.weights)) + self.visibleBias\n",
        "        \n",
        "        # Build up our mask for missing ratings\n",
        "        vMask = tf.sign(inputUser) # Make sure everything is 0 or 1\n",
        "        vMask3D = tf.reshape(vMask, [tf.shape(v)[0], -1, self.ratingValues]) # Reshape into arrays of individual ratings\n",
        "        vMask3D = tf.reduce_max(vMask3D, axis=[2], keepdims=True) # Use reduce_max to either give us 1 for ratings that exist, and 0 for missing ratings\n",
        "        \n",
        "        # Extract rating vectors for each individual set of 10 rating binary values\n",
        "        v = tf.reshape(v, [tf.shape(v)[0], -1, self.ratingValues])\n",
        "        vProb = tf.nn.softmax(v * vMask3D) # Apply softmax activation function\n",
        "        vProb = tf.reshape(vProb, [tf.shape(v)[0], -1]) # And shove them back into the flattened state. Reconstruction is done now.\n",
        "        # Stitch it together to define the backward pass and updated hidden biases\n",
        "        hProb1 = tf.nn.sigmoid(tf.matmul(vProb, self.weights) + self.hiddenBias)\n",
        "        backward = tf.matmul(tf.transpose(vProb), hProb1)\n",
        "    \n",
        "        # Now define what each epoch will do...\n",
        "        # Run the forward and backward passes, and update the weights\n",
        "        weightUpdate = self.weights.assign_add(self.learningRate * (forward - backward))\n",
        "        # Update hidden bias, minimizing the divergence in the hidden nodes\n",
        "        hiddenBiasUpdate = self.hiddenBias.assign_add(self.learningRate * tf.reduce_mean(hProb0 - hProb1, 0))\n",
        "        # Update the visible bias, minimizng divergence in the visible results\n",
        "        visibleBiasUpdate = self.visibleBias.assign_add(self.learningRate * tf.reduce_mean(inputUser - vProb, 0))\n",
        "\n",
        "        self.update = [weightUpdate, hiddenBiasUpdate, visibleBiasUpdate]\n",
        "        \n",
        "    def MakeHidden(self, inputUser):\n",
        "        hidden = tf.nn.sigmoid(tf.matmul(inputUser, self.weights) + self.hiddenBias)\n",
        "        self.MakeGraph(inputUser)\n",
        "        return hidden\n",
        "    \n",
        "    def MakeVisible(self, feed):\n",
        "        visible = tf.nn.sigmoid(tf.matmul(feed, tf.transpose(self.weights)) + self.visibleBias)\n",
        "        #self.MakeGraph(feed)\n",
        "        return visible\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSRNTBR8w5Ew",
        "outputId": "41051859-b203-47aa-d68e-8fccaa348a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.15.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1633709 sha256=dec6e9afe082064d7c79d73803690d42e105e3cffd55bb54026bf8ade010e4b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.1 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import AlgoBase\n",
        "from surprise import PredictionImpossible\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class RBMAlgorithm(AlgoBase):\n",
        "\n",
        "    def __init__(self, epochs=20, hiddenDim=100, learningRate=0.001, batchSize=100, sim_options={}):\n",
        "        AlgoBase.__init__(self)\n",
        "        self.epochs = epochs\n",
        "        self.hiddenDim = hiddenDim\n",
        "        self.learningRate = learningRate\n",
        "        self.batchSize = batchSize\n",
        "\n",
        "    def softmax(self, x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    def fit(self, trainset):\n",
        "        AlgoBase.fit(self, trainset)\n",
        "\n",
        "        numUsers = trainset.n_users\n",
        "        numItems = trainset.n_items\n",
        "\n",
        "        trainingMatrix = np.zeros([numUsers, numItems, 10], dtype=np.float32)\n",
        "\n",
        "        for (uid, iid, rating) in trainset.all_ratings():\n",
        "            adjustedRating = int(float(rating)*2.0) - 1\n",
        "            trainingMatrix[int(uid), int(iid), adjustedRating] = 1\n",
        "\n",
        "        # Flatten to a 2D array, with nodes for each possible rating type on each possible item, for every user.\n",
        "        trainingMatrix = np.reshape(trainingMatrix, [trainingMatrix.shape[0], -1])\n",
        "\n",
        "        # Create an RBM with (num items * rating values) visible nodes\n",
        "        rbm = RBMs(trainingMatrix.shape[1], hiddenDimensions=self.hiddenDim, learningRate=self.learningRate, batchSize=self.batchSize, epochs=self.epochs)\n",
        "        rbm.Train(trainingMatrix)\n",
        "\n",
        "        self.predictedRatings = np.zeros([numUsers, numItems], dtype=np.float32)\n",
        "        for uiid in range(trainset.n_users):\n",
        "            if (uiid % 50 == 0):\n",
        "                print(\"Processing user \", uiid)\n",
        "            recs = rbm.GetRecommendations([trainingMatrix[uiid]])\n",
        "            recs = np.reshape(recs, [numItems, 10])\n",
        "\n",
        "            for itemID, rec in enumerate(recs):\n",
        "                # The obvious thing would be to just take the rating with the highest score:\n",
        "                #rating = rec.argmax()\n",
        "                # ... but this just leads to a huge multi-way tie for 5-star predictions.\n",
        "                # The paper suggests performing normalization over K values to get probabilities\n",
        "                # and take the expectation as your prediction, so we'll do that instead:\n",
        "                normalized = self.softmax(rec)\n",
        "                rating = np.average(np.arange(10), weights=normalized)\n",
        "                self.predictedRatings[uiid, itemID] = (rating + 1) * 0.5\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def estimate(self, u, i):\n",
        "\n",
        "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
        "            raise PredictionImpossible('User and/or item is unkown.')\n",
        "\n",
        "        rating = self.predictedRatings[u, i]\n",
        "\n",
        "        if (rating < 0.001):\n",
        "            raise PredictionImpossible('No valid prediction exists.')\n",
        "\n",
        "        return rating"
      ],
      "metadata": {
        "id": "WxOsQVOywif4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qz3OL5pymog",
        "outputId": "28cf97cc-80ae-40ab-8483-604f4afff932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=30b917f0440df56b14d2d531450af03f32cfba5ef18cafdb2f270d14aedda48c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import sys\n",
        "import re\n",
        "import wget\n",
        "import zipfile\n",
        "\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "class MovieLens:\n",
        "\n",
        "    movieID_to_name = {}\n",
        "    name_to_movieID = {}\n",
        "    url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
        "    filename = wget.download(url)\n",
        "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "    ratingsPath =('ml-latest-small/ratings.csv')\n",
        "    moviesPath = 'ml-latest-small/movies.csv'\n",
        "    \n",
        "    def loadMovieLensLatestSmall(self):\n",
        "\n",
        "        # Look for files relative to the directory we are running from\n",
        "        os.chdir(os.path.dirname(sys.argv[0]))\n",
        "\n",
        "        ratingsDataset = 0\n",
        "        self.movieID_to_name = {}\n",
        "        self.name_to_movieID = {}\n",
        "\n",
        "        reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
        "\n",
        "        ratingsDataset = Dataset.load_from_file(self.ratingsPath, reader=reader)\n",
        "\n",
        "        with open(self.moviesPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "                movieReader = csv.reader(csvfile)\n",
        "                next(movieReader)  #Skip header line\n",
        "                for row in movieReader:\n",
        "                    movieID = int(row[0])\n",
        "                    movieName = row[1]\n",
        "                    self.movieID_to_name[movieID] = movieName\n",
        "                    self.name_to_movieID[movieName] = movieID\n",
        "\n",
        "        return ratingsDataset\n",
        "\n",
        "    def getUserRatings(self, user):\n",
        "        userRatings = []\n",
        "        hitUser = False\n",
        "        with open(self.ratingsPath, newline='') as csvfile:\n",
        "            ratingReader = csv.reader(csvfile)\n",
        "            next(ratingReader)\n",
        "            for row in ratingReader:\n",
        "                userID = int(row[0])\n",
        "                if (user == userID):\n",
        "                    movieID = int(row[1])\n",
        "                    rating = float(row[2])\n",
        "                    userRatings.append((movieID, rating))\n",
        "                    hitUser = True\n",
        "                if (hitUser and (user != userID)):\n",
        "                    break\n",
        "\n",
        "        return userRatings\n",
        "\n",
        "    def getPopularityRanks(self):\n",
        "        ratings = defaultdict(int)\n",
        "        rankings = defaultdict(int)\n",
        "        with open(self.ratingsPath, newline='') as csvfile:\n",
        "            ratingReader = csv.reader(csvfile)\n",
        "            next(ratingReader)\n",
        "            for row in ratingReader:\n",
        "                movieID = int(row[1])\n",
        "                ratings[movieID] += 1\n",
        "        rank = 1\n",
        "        for movieID, ratingCount in sorted(ratings.items(), key=lambda x: x[1], reverse=True):\n",
        "            rankings[movieID] = rank\n",
        "            rank += 1\n",
        "        return rankings\n",
        "\n",
        "    def getGenres(self):\n",
        "        genres = defaultdict(list)\n",
        "        genreIDs = {}\n",
        "        maxGenreID = 0\n",
        "        with open(self.moviesPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "            movieReader = csv.reader(csvfile)\n",
        "            next(movieReader)  #Skip header line\n",
        "            for row in movieReader:\n",
        "                movieID = int(row[0])\n",
        "                genreList = row[2].split('|')\n",
        "                genreIDList = []\n",
        "                for genre in genreList:\n",
        "                    if genre in genreIDs:\n",
        "                        genreID = genreIDs[genre]\n",
        "                    else:\n",
        "                        genreID = maxGenreID\n",
        "                        genreIDs[genre] = genreID\n",
        "                        maxGenreID += 1\n",
        "                    genreIDList.append(genreID)\n",
        "                genres[movieID] = genreIDList\n",
        "        # Convert integer-encoded genre lists to bitfields that we can treat as vectors\n",
        "        for (movieID, genreIDList) in genres.items():\n",
        "            bitfield = [0] * maxGenreID\n",
        "            for genreID in genreIDList:\n",
        "                bitfield[genreID] = 1\n",
        "            genres[movieID] = bitfield\n",
        "\n",
        "        return genres\n",
        "\n",
        "    def getYears(self):\n",
        "        p = re.compile(r\"(?:\\((\\d{4})\\))?\\s*$\")\n",
        "        years = defaultdict(int)\n",
        "        with open(self.moviesPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "            movieReader = csv.reader(csvfile)\n",
        "            next(movieReader)\n",
        "            for row in movieReader:\n",
        "                movieID = int(row[0])\n",
        "                title = row[1]\n",
        "                m = p.search(title)\n",
        "                year = m.group(1)\n",
        "                if year:\n",
        "                    years[movieID] = int(year)\n",
        "        return years\n",
        "\n",
        "    def getMiseEnScene(self):\n",
        "        mes = defaultdict(list)\n",
        "        with open(\"LLVisualFeatures13K_Log.csv\", newline='') as csvfile:\n",
        "            mesReader = csv.reader(csvfile)\n",
        "            next(mesReader)\n",
        "            for row in mesReader:\n",
        "                movieID = int(row[0])\n",
        "                avgShotLength = float(row[1])\n",
        "                meanColorVariance = float(row[2])\n",
        "                stddevColorVariance = float(row[3])\n",
        "                meanMotion = float(row[4])\n",
        "                stddevMotion = float(row[5])\n",
        "                meanLightingKey = float(row[6])\n",
        "                numShots = float(row[7])\n",
        "                mes[movieID] = [avgShotLength, meanColorVariance, stddevColorVariance,\n",
        "                   meanMotion, stddevMotion, meanLightingKey, numShots]\n",
        "        return mes\n",
        "\n",
        "    def getMovieName(self, movieID):\n",
        "        if movieID in self.movieID_to_name:\n",
        "            return self.movieID_to_name[movieID]\n",
        "        else:\n",
        "            return \"\"\n",
        "\n",
        "    def getMovieID(self, movieName):\n",
        "        if movieName in self.name_to_movieID:\n",
        "            return self.name_to_movieID[movieName]\n",
        "        else:\n",
        "            return 0"
      ],
      "metadata": {
        "id": "IW7-mmpJxH0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise.model_selection import train_test_split\n",
        "from surprise.model_selection import LeaveOneOut\n",
        "from surprise import KNNBaseline\n",
        "\n",
        "class EvaluationData:\n",
        "\n",
        "    def __init__(self, data, popularityRankings):\n",
        "\n",
        "        self.rankings = popularityRankings\n",
        "\n",
        "        #Build a full training set for evaluating overall properties\n",
        "        self.fullTrainSet = data.build_full_trainset()\n",
        "        self.fullAntiTestSet = self.fullTrainSet.build_anti_testset()\n",
        "\n",
        "        #Build a 75/25 train/test split for measuring accuracy\n",
        "        self.trainSet, self.testSet = train_test_split(data, test_size=.25, random_state=1)\n",
        "\n",
        "        #Build a \"leave one out\" train/test split for evaluating top-N recommenders\n",
        "        #And build an anti-test-set for building predictions\n",
        "        LOOCV = LeaveOneOut(n_splits=1, random_state=1)\n",
        "        for train, test in LOOCV.split(data):\n",
        "            self.LOOCVTrain = train\n",
        "            self.LOOCVTest = test\n",
        "\n",
        "        self.LOOCVAntiTestSet = self.LOOCVTrain.build_anti_testset()\n",
        "\n",
        "        #Compute similarty matrix between items so we can measure diversity\n",
        "        sim_options = {'name': 'cosine', 'user_based': False}\n",
        "        self.simsAlgo = KNNBaseline(sim_options=sim_options)\n",
        "        self.simsAlgo.fit(self.fullTrainSet)\n",
        "\n",
        "    def GetFullTrainSet(self):\n",
        "        return self.fullTrainSet\n",
        "\n",
        "    def GetFullAntiTestSet(self):\n",
        "        return self.fullAntiTestSet\n",
        "\n",
        "    def GetAntiTestSetForUser(self, testSubject):\n",
        "        trainset = self.fullTrainSet\n",
        "        fill = trainset.global_mean\n",
        "        anti_testset = []\n",
        "        u = trainset.to_inner_uid(str(testSubject))\n",
        "        user_items = set([j for (j, _) in trainset.ur[u]])\n",
        "        anti_testset += [(trainset.to_raw_uid(u), trainset.to_raw_iid(i), fill) for\n",
        "                                 i in trainset.all_items() if\n",
        "                                 i not in user_items]\n",
        "        return anti_testset\n",
        "\n",
        "    def GetTrainSet(self):\n",
        "        return self.trainSet\n",
        "\n",
        "    def GetTestSet(self):\n",
        "        return self.testSet\n",
        "\n",
        "    def GetLOOCVTrainSet(self):\n",
        "        return self.LOOCVTrain\n",
        "\n",
        "    def GetLOOCVTestSet(self):\n",
        "        return self.LOOCVTest\n",
        "\n",
        "    def GetLOOCVAntiTestSet(self):\n",
        "        return self.LOOCVAntiTestSet\n",
        "\n",
        "    def GetSimilarities(self):\n",
        "        return self.simsAlgo\n",
        "\n",
        "    def GetPopularityRankings(self):\n",
        "        return self.rankings"
      ],
      "metadata": {
        "id": "i7B6aXEVxIBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "from surprise import accuracy\n",
        "from collections import defaultdict\n",
        "\n",
        "class RecommenderMetrics:\n",
        "\n",
        "    def MAE(predictions):\n",
        "        return accuracy.mae(predictions, verbose=False)\n",
        "\n",
        "    def RMSE(predictions):\n",
        "        return accuracy.rmse(predictions, verbose=False)\n",
        "\n",
        "    def GetTopN(predictions, n=10, minimumRating=0.0):\n",
        "        topN = defaultdict(list)\n",
        "\n",
        "\n",
        "        for userID, movieID, actualRating, estimatedRating, _ in predictions:\n",
        "            if (estimatedRating >= minimumRating):\n",
        "                topN[int(userID)].append((int(movieID), estimatedRating))\n",
        "\n",
        "        for userID, ratings in topN.items():\n",
        "            ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "            topN[int(userID)] = ratings[:n]\n",
        "\n",
        "        return topN\n",
        "\n",
        "    def HitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        # For each left-out rating\n",
        "        for leftOut in leftOutPredictions:\n",
        "            userID = leftOut[0]\n",
        "            leftOutMovieID = leftOut[1]\n",
        "            # Is it in the predicted top 10 for this user?\n",
        "            hit = False\n",
        "            for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                if (int(leftOutMovieID) == int(movieID)):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit) :\n",
        "                hits += 1\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        return hits/total\n",
        "\n",
        "    def CumulativeHitRate(topNPredicted, leftOutPredictions, ratingCutoff=0):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Only look at ability to recommend things the users actually liked...\n",
        "            if (actualRating >= ratingCutoff):\n",
        "                # Is it in the predicted top 10 for this user?\n",
        "                hit = False\n",
        "                for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                    if (int(leftOutMovieID) == movieID):\n",
        "                        hit = True\n",
        "                        break\n",
        "                if (hit) :\n",
        "                    hits += 1\n",
        "\n",
        "                total += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        return hits/total\n",
        "\n",
        "    def RatingHitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = defaultdict(float)\n",
        "        total = defaultdict(float)\n",
        "\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Is it in the predicted top N for this user?\n",
        "            hit = False\n",
        "            for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                if (int(leftOutMovieID) == movieID):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit) :\n",
        "                hits[actualRating] += 1\n",
        "\n",
        "            total[actualRating] += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        for rating in sorted(hits.keys()):\n",
        "            print (rating, hits[rating] / total[rating])\n",
        "\n",
        "    def AverageReciprocalHitRank(topNPredicted, leftOutPredictions):\n",
        "        summation = 0\n",
        "        total = 0\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Is it in the predicted top N for this user?\n",
        "            hitRank = 0\n",
        "            rank = 0\n",
        "            for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                rank = rank + 1\n",
        "                if (int(leftOutMovieID) == movieID):\n",
        "                    hitRank = rank\n",
        "                    break\n",
        "            if (hitRank > 0) :\n",
        "                summation += 1.0 / hitRank\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        return summation / total\n",
        "\n",
        "    # What percentage of users have at least one \"good\" recommendation\n",
        "    def UserCoverage(topNPredicted, numUsers, ratingThreshold=0):\n",
        "        hits = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            hit = False\n",
        "            for movieID, predictedRating in topNPredicted[userID]:\n",
        "                if (predictedRating >= ratingThreshold):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit):\n",
        "                hits += 1\n",
        "\n",
        "        return hits / numUsers\n",
        "\n",
        "    def Diversity(topNPredicted, simsAlgo):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        simsMatrix = simsAlgo.compute_similarities()\n",
        "        for userID in topNPredicted.keys():\n",
        "            pairs = itertools.combinations(topNPredicted[userID], 2)\n",
        "            for pair in pairs:\n",
        "                movie1 = pair[0][0]\n",
        "                movie2 = pair[1][0]\n",
        "                innerID1 = simsAlgo.trainset.to_inner_iid(str(movie1))\n",
        "                innerID2 = simsAlgo.trainset.to_inner_iid(str(movie2))\n",
        "                similarity = simsMatrix[innerID1][innerID2]\n",
        "                total += similarity\n",
        "                n += 1\n",
        "\n",
        "        if (n > 0):\n",
        "            S = total / n\n",
        "            return (1-S)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def Novelty(topNPredicted, rankings):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            for rating in topNPredicted[userID]:\n",
        "                movieID = rating[0]\n",
        "                rank = rankings[movieID]\n",
        "                total += rank\n",
        "                n += 1\n",
        "        return total / n"
      ],
      "metadata": {
        "id": "RQskj5MExV9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EvaluatedAlgorithm:\n",
        "\n",
        "    def __init__(self, algorithm, name):\n",
        "        self.algorithm = algorithm\n",
        "        self.name = name\n",
        "\n",
        "    def Evaluate(self, evaluationData, doTopN, n=10, verbose=True):\n",
        "        metrics = {}\n",
        "        # Compute accuracy\n",
        "        if (verbose):\n",
        "            print(\"Evaluating accuracy...\")\n",
        "        self.algorithm.fit(evaluationData.GetTrainSet())\n",
        "        predictions = self.algorithm.test(evaluationData.GetTestSet())\n",
        "        metrics[\"RMSE\"] = RecommenderMetrics.RMSE(predictions)\n",
        "        metrics[\"MAE\"] = RecommenderMetrics.MAE(predictions)\n",
        "\n",
        "        if (doTopN):\n",
        "            # Evaluate top-10 with Leave One Out testing\n",
        "            if (verbose):\n",
        "                print(\"Evaluating top-N with leave-one-out...\")\n",
        "            self.algorithm.fit(evaluationData.GetLOOCVTrainSet())\n",
        "            leftOutPredictions = self.algorithm.test(evaluationData.GetLOOCVTestSet())\n",
        "            # Build predictions for all ratings not in the training set\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetLOOCVAntiTestSet())\n",
        "            # Compute top 10 recs for each user\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Computing hit-rate and rank metrics...\")\n",
        "            # See how often we recommended a movie the user actually rated\n",
        "            metrics[\"HR\"] = RecommenderMetrics.HitRate(topNPredicted, leftOutPredictions)\n",
        "            # See how often we recommended a movie the user actually liked\n",
        "            metrics[\"cHR\"] = RecommenderMetrics.CumulativeHitRate(topNPredicted, leftOutPredictions)\n",
        "            # Compute ARHR\n",
        "            metrics[\"ARHR\"] = RecommenderMetrics.AverageReciprocalHitRank(topNPredicted, leftOutPredictions)\n",
        "\n",
        "            #Evaluate properties of recommendations on full training set\n",
        "            if (verbose):\n",
        "                print(\"Computing recommendations with full data set...\")\n",
        "            self.algorithm.fit(evaluationData.GetFullTrainSet())\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetFullAntiTestSet())\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Analyzing coverage, diversity, and novelty...\")\n",
        "            # Print user coverage with a minimum predicted rating of 4.0:\n",
        "            metrics[\"Coverage\"] = RecommenderMetrics.UserCoverage(  topNPredicted,\n",
        "                                                                   evaluationData.GetFullTrainSet().n_users,\n",
        "                                                                   ratingThreshold=4.0)\n",
        "            # Measure diversity of recommendations:\n",
        "            metrics[\"Diversity\"] = RecommenderMetrics.Diversity(topNPredicted, evaluationData.GetSimilarities())\n",
        "\n",
        "            # Measure novelty (average popularity rank of recommendations):\n",
        "            metrics[\"Novelty\"] = RecommenderMetrics.Novelty(topNPredicted,\n",
        "                                                            evaluationData.GetPopularityRankings())\n",
        "\n",
        "        if (verbose):\n",
        "            print(\"Analysis complete.\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def GetName(self):\n",
        "        return self.name\n",
        "\n",
        "    def GetAlgorithm(self):\n",
        "        return self.algorithm"
      ],
      "metadata": {
        "id": "G3eHXiUHxd61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator:\n",
        "\n",
        "    algorithms = []\n",
        "\n",
        "    def __init__(self, dataset, rankings):\n",
        "        ed = EvaluationData(dataset, rankings)\n",
        "        self.dataset = ed\n",
        "\n",
        "    def AddAlgorithm(self, algorithm, name):\n",
        "        alg = EvaluatedAlgorithm(algorithm, name)\n",
        "        self.algorithms.append(alg)\n",
        "\n",
        "    def Evaluate(self, doTopN):\n",
        "        results = {}\n",
        "        for algorithm in self.algorithms:\n",
        "            print(\"Evaluating \", algorithm.GetName(), \"...\")\n",
        "            results[algorithm.GetName()] = algorithm.Evaluate(self.dataset, doTopN)\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if (doTopN):\n",
        "            print(\"{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
        "                    \"Algorithm\", \"RMSE\", \"MAE\", \"HR\", \"cHR\", \"ARHR\", \"Coverage\", \"Diversity\", \"Novelty\"))\n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"{:<10} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
        "                        name, metrics[\"RMSE\"], metrics[\"MAE\"], metrics[\"HR\"], metrics[\"cHR\"], metrics[\"ARHR\"],\n",
        "                                      metrics[\"Coverage\"], metrics[\"Diversity\"], metrics[\"Novelty\"]))\n",
        "        else:\n",
        "            print(\"{:<10} {:<10} {:<10}\".format(\"Algorithm\", \"RMSE\", \"MAE\"))\n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"{:<10} {:<10.4f} {:<10.4f}\".format(name, metrics[\"RMSE\"], metrics[\"MAE\"]))\n",
        "\n",
        "        print(\"\\nLegend:\\n\")\n",
        "        print(\"RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\")\n",
        "        print(\"MAE:       Mean Absolute Error. Lower values mean better accuracy.\")\n",
        "        if (doTopN):\n",
        "            print(\"HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\")\n",
        "            print(\"cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\")\n",
        "            print(\"ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\" )\n",
        "            print(\"Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\")\n",
        "            print(\"Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\")\n",
        "            print(\"           for a given user. Higher means more diverse.\")\n",
        "            print(\"Novelty:   Average popularity rank of recommended items. Higher means more novel.\")\n",
        "\n",
        "    def SampleTopNRecs(self, ml, testSubject=85, k=10):\n",
        "\n",
        "        for algo in self.algorithms:\n",
        "            print(\"\\nUsing recommender \", algo.GetName())\n",
        "\n",
        "            print(\"\\nBuilding recommendation model...\")\n",
        "            trainSet = self.dataset.GetFullTrainSet()\n",
        "            algo.GetAlgorithm().fit(trainSet)\n",
        "\n",
        "            print(\"Computing recommendations...\")\n",
        "            testSet = self.dataset.GetAntiTestSetForUser(testSubject)\n",
        "\n",
        "            predictions = algo.GetAlgorithm().test(testSet)\n",
        "\n",
        "            recommendations = []\n",
        "\n",
        "            print (\"\\nWe recommend:\")\n",
        "            for userID, movieID, actualRating, estimatedRating, _ in predictions:\n",
        "                intMovieID = int(movieID)\n",
        "                recommendations.append((intMovieID, estimatedRating))\n",
        "\n",
        "            recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            for ratings in recommendations[:10]:\n",
        "                print(ml.getMovieName(ratings[0]), ratings[1])\n",
        "                "
      ],
      "metadata": {
        "id": "UflF8XXDxjBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import NormalPredictor\n",
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "import random\n",
        "\n",
        "def LoadMovieLensData():\n",
        "    ml = MovieLens()\n",
        "    print(\"Loading movie ratings...\")\n",
        "    data = ml.loadMovieLensLatestSmall()\n",
        "    print(\"\\nComputing movie popularity ranks so we can measure novelty later...\")\n",
        "    rankings = ml.getPopularityRanks()\n",
        "    return (ml, data, rankings)\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# Load up common data set for the recommender algorithms\n",
        "(ml, evaluationData, rankings) = LoadMovieLensData()\n",
        "\n",
        "print(\"Searching for best parameters...\")\n",
        "param_grid = {'hiddenDim': [20, 10], 'learningRate': [0.1, 0.01]}\n",
        "gs = GridSearchCV(RBMAlgorithm, param_grid, measures=['rmse', 'mae'], cv=3)\n",
        "\n",
        "gs.fit(evaluationData)\n",
        "\n",
        "# best RMSE score\n",
        "print(\"Best RMSE score attained: \", gs.best_score['rmse'])\n",
        "\n",
        "# combination of parameters that gave the best RMSE score\n",
        "print(gs.best_params['rmse'])\n",
        "\n",
        "# Construct an Evaluator to, you know, evaluate them\n",
        "evaluator = Evaluator(evaluationData, rankings)\n",
        "\n",
        "params = gs.best_params['rmse']\n",
        "RBMtuned = RBMAlgorithm(hiddenDim = params['hiddenDim'], learningRate = params['learningRate'])\n",
        "evaluator.AddAlgorithm(RBMtuned, \"RBM - Tuned\")\n",
        "\n",
        "RBMUntuned = RBMAlgorithm()\n",
        "evaluator.AddAlgorithm(RBMUntuned, \"RBM - Untuned\")\n",
        "\n",
        "# Just make random recommendations\n",
        "Random = NormalPredictor()\n",
        "evaluator.AddAlgorithm(Random, \"Random\")\n",
        "\n",
        "# Fight!\n",
        "evaluator.Evaluate(True)\n",
        "\n",
        "evaluator.SampleTopNRecs(ml)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnf7v6nRxoTs",
        "outputId": "07151542-ef75-4534-9378-43c1a9ca6ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading movie ratings...\n",
            "\n",
            "Computing movie popularity ranks so we can measure novelty later...\n",
            "Searching for best parameters...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Best RMSE score attained:  1.2821291522709233\n",
            "{'hiddenDim': 20, 'learningRate': 0.1}\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Evaluating  RBM - Tuned ...\n",
            "Evaluating accuracy...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Evaluating top-N with leave-one-out...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Computing hit-rate and rank metrics...\n",
            "Computing recommendations with full data set...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Analyzing coverage, diversity, and novelty...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Analysis complete.\n",
            "Evaluating  RBM - Untuned ...\n",
            "Evaluating accuracy...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Evaluating top-N with leave-one-out...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Computing hit-rate and rank metrics...\n",
            "Computing recommendations with full data set...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Analyzing coverage, diversity, and novelty...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Analysis complete.\n",
            "Evaluating  Random ...\n",
            "Evaluating accuracy...\n",
            "Evaluating top-N with leave-one-out...\n",
            "Computing hit-rate and rank metrics...\n",
            "Computing recommendations with full data set...\n",
            "Analyzing coverage, diversity, and novelty...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Analysis complete.\n",
            "\n",
            "\n",
            "Algorithm  RMSE       MAE        HR         cHR        ARHR       Coverage   Diversity  Novelty   \n",
            "RBM - Tuned 1.2808     1.0867     0.0164     0.0164     0.0035     0.0000     0.0716     1085.6805 \n",
            "RBM - Untuned 1.2814     1.0872     0.0033     0.0033     0.0011     0.0000     0.7367     4930.0256 \n",
            "Random     1.4257     1.1367     0.0098     0.0098     0.0046     1.0000     0.0493     845.8962  \n",
            "\n",
            "Legend:\n",
            "\n",
            "RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\n",
            "MAE:       Mean Absolute Error. Lower values mean better accuracy.\n",
            "HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\n",
            "cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\n",
            "ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\n",
            "Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\n",
            "Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\n",
            "           for a given user. Higher means more diverse.\n",
            "Novelty:   Average popularity rank of recommended items. Higher means more novel.\n",
            "\n",
            "Using recommender  RBM - Tuned\n",
            "\n",
            "Building recommendation model...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Computing recommendations...\n",
            "\n",
            "We recommend:\n",
            "South Park: Bigger, Longer and Uncut (1999) 2.7935112\n",
            "American Beauty (1999) 2.7898998\n",
            "Run Lola Run (Lola rennt) (1998) 2.7890286\n",
            "Sixth Sense, The (1999) 2.7882574\n",
            "Election (1999) 2.7865226\n",
            "Muse, The (1999) 2.7740316\n",
            "Summer of Sam (1999) 2.7715278\n",
            "American Pie (1999) 2.770767\n",
            "Matrix, The (1999) 2.7699697\n",
            "Autumn Tale, An (Conte d'automne) (1998) 2.764256\n",
            "\n",
            "Using recommender  RBM - Untuned\n",
            "\n",
            "Building recommendation model...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Computing recommendations...\n",
            "\n",
            "We recommend:\n",
            "Spartacus (1960) 2.7863925\n",
            "Fugitive, The (1947) 2.7848783\n",
            "Planet of the Apes (2001) 2.7824247\n",
            "Death Race (2008) 2.7818868\n",
            "Raining Stones (1993) 2.7818503\n",
            "Eyes of Tammy Faye, The (2000) 2.7814822\n",
            "Starship Troopers 2: Hero of the Federation (2004) 2.781226\n",
            "American Fable (2017) 2.780959\n",
            "Blackboard Jungle (1955) 2.7807229\n",
            "Freaks of Nature (2015) 2.7806444\n",
            "\n",
            "Using recommender  Random\n",
            "\n",
            "Building recommendation model...\n",
            "Computing recommendations...\n",
            "\n",
            "We recommend:\n",
            "Clerks (1994) 5\n",
            "Jungle Book, The (1994) 5\n",
            "Jurassic Park (1993) 5\n",
            "Batman (1989) 5\n",
            "Space Jam (1996) 5\n",
            "Twister (1996) 5\n",
            "E.T. the Extra-Terrestrial (1982) 5\n",
            "Monty Python and the Holy Grail (1975) 5\n",
            "Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981) 5\n",
            "Goodfellas (1990) 5\n"
          ]
        }
      ]
    }
  ]
}